#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬
1. ä»æœ¬åœ°æ•°æ®åº“å¯¼å‡ºæ•°æ®åˆ°Supabase
2. éªŒè¯æ•°æ®å®Œæ•´æ€§
3. å‡†å¤‡ç”Ÿäº§ç¯å¢ƒé…ç½®
"""

import os
import sys
import json
from datetime import datetime
from sqlalchemy import create_engine, text
import requests

def log(message, level="INFO"):
    """æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {level}: {message}")

def check_local_database():
    """æ£€æŸ¥æœ¬åœ°æ•°æ®åº“è¿æ¥å’Œæ•°æ®"""
    log("æ£€æŸ¥æœ¬åœ°æ•°æ®åº“...")
    
    try:
        local_db_url = 'postgresql://postgres:Qaz961264727@localhost/cruise_system_development'
        local_engine = create_engine(local_db_url)
        
        with local_engine.connect() as conn:
            # æ£€æŸ¥å…³é”®è¡¨çš„æ•°æ®é‡
            tables_to_check = ['users', 'products', 'suppliers', 'categories', 'countries']
            data_summary = {}
            
            for table in tables_to_check:
                result = conn.execute(text(f'SELECT COUNT(*) FROM {table}'))
                count = result.fetchone()[0]
                data_summary[table] = count
                log(f"  {table}: {count} æ¡è®°å½•")
            
            return data_summary
            
    except Exception as e:
        log(f"æœ¬åœ°æ•°æ®åº“è¿æ¥å¤±è´¥: {e}", "ERROR")
        return None

def export_table_data(table_name, columns=None):
    """å¯¼å‡ºè¡¨æ•°æ®ä¸ºINSERTè¯­å¥"""
    log(f"å¯¼å‡º {table_name} è¡¨æ•°æ®...")
    
    try:
        local_db_url = 'postgresql://postgres:Qaz961264727@localhost/cruise_system_development'
        local_engine = create_engine(local_db_url)
        
        with local_engine.connect() as conn:
            if columns:
                column_list = ', '.join(columns)
                query = f'SELECT {column_list} FROM {table_name} ORDER BY id'
            else:
                query = f'SELECT * FROM {table_name} ORDER BY id'
            
            result = conn.execute(text(query))
            rows = result.fetchall()
            
            if not rows:
                log(f"  {table_name} è¡¨ä¸ºç©º")
                return []
            
            # è·å–åˆ—å
            if columns:
                col_names = columns
            else:
                # è·å–è¡¨ç»“æ„
                schema_result = conn.execute(text(f'''
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = '{table_name}' 
                    ORDER BY ordinal_position
                '''))
                col_names = [row[0] for row in schema_result.fetchall()]
            
            # ç”ŸæˆINSERTè¯­å¥
            insert_statements = []
            for row in rows:
                values = []
                for val in row:
                    if val is None:
                        values.append('NULL')
                    elif isinstance(val, str):
                        # è½¬ä¹‰å•å¼•å·
                        escaped_val = val.replace("'", "''")
                        values.append(f"'{escaped_val}'")
                    elif isinstance(val, bool):
                        values.append('true' if val else 'false')
                    elif isinstance(val, datetime):
                        values.append(f"'{val.isoformat()}'")
                    else:
                        values.append(str(val))
                
                columns_str = ', '.join(col_names)
                values_str = ', '.join(values)
                insert_stmt = f"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str});"
                insert_statements.append(insert_stmt)
            
            log(f"  å¯¼å‡º {len(insert_statements)} æ¡ {table_name} è®°å½•")
            return insert_statements
            
    except Exception as e:
        log(f"å¯¼å‡º {table_name} å¤±è´¥: {e}", "ERROR")
        return []

def create_migration_script():
    """åˆ›å»ºå®Œæ•´çš„æ•°æ®è¿ç§»è„šæœ¬"""
    log("åˆ›å»ºæ•°æ®è¿ç§»è„šæœ¬...")
    
    # å®šä¹‰è¦è¿ç§»çš„è¡¨å’Œå­—æ®µ
    tables_to_migrate = {
        'countries': ['name', 'code', 'status', 'created_at', 'updated_at'],
        'categories': ['name', 'description', 'status', 'created_at', 'updated_at'],
        'suppliers': ['name', 'contact', 'email', 'phone', 'address', 'status', 'created_at', 'updated_at'],
        'products': [
            'product_name_en', 'product_name_jp', 'code', 'country_id', 'category_id', 
            'supplier_id', 'port_id', 'unit', 'price', 'unit_size', 'pack_size', 
            'country_of_origin', 'brand', 'currency', 'effective_from', 'effective_to', 
            'status', 'created_at', 'updated_at'
        ]
    }
    
    migration_script = []
    migration_script.append("-- ç”Ÿäº§ç¯å¢ƒæ•°æ®è¿ç§»è„šæœ¬")
    migration_script.append(f"-- ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}")
    migration_script.append("-- æ³¨æ„: æ­¤è„šæœ¬å°†æ¸…ç©ºç°æœ‰æ•°æ®å¹¶é‡æ–°å¯¼å…¥")
    migration_script.append("")
    
    # æŒ‰ä¾èµ–é¡ºåºè¿ç§»è¡¨
    for table_name, columns in tables_to_migrate.items():
        migration_script.append(f"-- è¿ç§» {table_name} è¡¨")
        migration_script.append(f"DELETE FROM {table_name};")
        migration_script.append(f"ALTER SEQUENCE {table_name}_id_seq RESTART WITH 1;")
        migration_script.append("")
        
        # å¯¼å‡ºæ•°æ®
        insert_statements = export_table_data(table_name, columns)
        migration_script.extend(insert_statements)
        migration_script.append("")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    script_content = '\n'.join(migration_script)
    with open('production_migration.sql', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    log(f"âœ… è¿ç§»è„šæœ¬å·²ä¿å­˜åˆ° production_migration.sql")
    return 'production_migration.sql'

def test_supabase_connection():
    """æµ‹è¯•Supabaseè¿æ¥"""
    log("æµ‹è¯•Supabaseè¿æ¥...")
    
    try:
        from app.core.config import settings
        from app.db.session import SessionLocal
        
        # æµ‹è¯•è¿æ¥
        db = SessionLocal()
        result = db.execute(text('SELECT version()'))
        version = result.fetchone()[0]
        log(f"âœ… Supabaseè¿æ¥æˆåŠŸ!")
        log(f"  PostgreSQLç‰ˆæœ¬: {version.split(',')[0]}")
        
        # æ£€æŸ¥è¡¨
        result = db.execute(text("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_name"))
        tables = [row[0] for row in result.fetchall()]
        log(f"  æ‰¾åˆ° {len(tables)} ä¸ªè¡¨: {', '.join(tables)}")
        
        db.close()
        return True
        
    except Exception as e:
        log(f"âŒ Supabaseè¿æ¥å¤±è´¥: {e}", "ERROR")
        return False

def create_production_env():
    """åˆ›å»ºç”Ÿäº§ç¯å¢ƒé…ç½®æ–‡ä»¶"""
    log("åˆ›å»ºç”Ÿäº§ç¯å¢ƒé…ç½®...")
    
    production_config = """# ç”Ÿäº§ç¯å¢ƒé…ç½®
ENV=production

# Supabaseæ•°æ®åº“è¿æ¥
SUPABASE_DB_URL=postgresql://postgres.yczgwmbnjomhrvxtuupt:Qaz2465672485@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres?sslmode=require
SUPABASE_URL=https://yczgwmbnjomhrvxtuupt.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inljemd3bWJuam9taHJ2eHR1dXB0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDM2NjQ3NjMsImV4cCI6MjA1OTI0MDc2M30.IGGBZhIXwoCPyN7gUjQIbn2tSfA-SSB9EnPWCH_cWbI

# JWTè®¾ç½®
SECRET_KEY=pdT0M5o4lzSXK6ZxUQsaA2WDI9YjN3cF1hkLrVgwOPeJ

# é»˜è®¤è¶…çº§ç®¡ç†å‘˜
FIRST_SUPERADMIN=admin@example.com
FIRST_SUPERADMIN_PASSWORD=adminpassword

# SMTPè®¾ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒéœ€è¦é…ç½®çœŸå®é‚®ç®±ï¼‰
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
"""
    
    with open('.env.production', 'w', encoding='utf-8') as f:
        f.write(production_config)
    
    log("âœ… ç”Ÿäº§ç¯å¢ƒé…ç½®å·²ä¿å­˜åˆ° .env.production")

def create_docker_files():
    """åˆ›å»ºDockeréƒ¨ç½²æ–‡ä»¶"""
    log("åˆ›å»ºDockeréƒ¨ç½²æ–‡ä»¶...")
    
    # Dockerfile
    dockerfile_content = """FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    gcc \\
    libpq-dev \\
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PORT=8080

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
"""
    
    with open('Dockerfile', 'w') as f:
        f.write(dockerfile_content)
    
    # .dockerignore
    dockerignore_content = """__pycache__
*.pyc
*.pyo
*.pyd
.Python
env
pip-log.txt
pip-delete-this-directory.txt
.tox
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.log
.git
.mypy_cache
.pytest_cache
.hypothesis
.venv
venv/
.env
.env.local
.env.development
uploads/
logs/
test.db
*.sql
docs/
scripts/
tests/
"""
    
    with open('.dockerignore', 'w') as f:
        f.write(dockerignore_content)
    
    log("âœ… Dockeræ–‡ä»¶å·²åˆ›å»º")

def main():
    """ä¸»å‡½æ•°"""
    log("ğŸš€ å¼€å§‹ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‡†å¤‡...")
    
    # 1. æ£€æŸ¥æœ¬åœ°æ•°æ®åº“
    local_data = check_local_database()
    if not local_data:
        log("æœ¬åœ°æ•°æ®åº“æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º", "ERROR")
        return False
    
    # 2. æµ‹è¯•Supabaseè¿æ¥
    if not test_supabase_connection():
        log("Supabaseè¿æ¥å¤±è´¥ï¼Œé€€å‡º", "ERROR")
        return False
    
    # 3. åˆ›å»ºæ•°æ®è¿ç§»è„šæœ¬
    migration_file = create_migration_script()
    
    # 4. åˆ›å»ºç”Ÿäº§ç¯å¢ƒé…ç½®
    create_production_env()
    
    # 5. åˆ›å»ºDockeræ–‡ä»¶
    create_docker_files()
    
    log("âœ… ç”Ÿäº§ç¯å¢ƒå‡†å¤‡å®Œæˆ!")
    log("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    log("  1. æ‰§è¡Œæ•°æ®è¿ç§»: ä½¿ç”¨Supabase MCPæ‰§è¡Œ production_migration.sql")
    log("  2. æ„å»ºDockeré•œåƒ: docker build -t cruise-backend .")
    log("  3. éƒ¨ç½²åˆ°Google Cloud Run")
    log("  4. éƒ¨ç½²å‰ç«¯åˆ°Vercel")
    
    return True

if __name__ == "__main__":
    main()
